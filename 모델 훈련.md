본론에 앞서서..
* 확률? 가능도? 총가능도? 최대우도추정? 최대사후확률추정?
    * 확률(Probability)
        * **모수(parameter)가 정해진 특정 확률분포**가 있을 때, 관측값 혹은 관측 구간이 분포 안에서 빈도를 나타내는 값
    * 가능도(= 우도, likelihood)
        * **관측값이 주어졌을 때**, 어떤 확률 분포에서 왔을 지에 대한 발생 가능성을 나타내는 값
    * 총 가능도
        * 모든 가능도의 곱한 값
    * 최대 우도 추정(MLE, Maximum Likelihood Estimation)
        * **빈도론 관점**에서 최적의 모수를 찾아내는 방법
        * 주어진 관측값(들)의 발생 가능성(우도 또는 총 가능도)이 가장 높은 모수를 추정하는 방법으로, 이를 통해 최적의 확률분포를 도출 해낼 수 있음
        * 우도를 구하기 위해 관측결과에 대해서 모델링(특정 확률분포를 가정)을 진행하고, 우도(또는 총 가능도)가 최대가 되는 모수를 미분 또는 경사하강법 등 수학적 기법으로 구함
        * 주어진 관측 결과를 바탕으로 쉽게 진행 할 수 있는 장점이 있지만, 관측결과에 의존해서 판단하기 때문에 관측결과의 품질(관측횟수, 정확성)에 따라서 모수 추정의 성패가 크게 좌우됨
    * 최대사후확률추정(MAP, MAximum Posteriori estimation)
        * **베이지안 관점**에서 최적의 모수를 찾아내는 관점
        * 주어진 관측결과에 대한 우도와 사전확률을 결합해서 최적의 모수를 찾아내는 방법
            * 베이즈 정리에 의해 사후확률은 사전확률과 관측결과에 대한 우도의 곱에 비례한다는 것을 활용함
        * 관측결과에 대한 모델링과 사전확률에 대한 모델링이 필요함
        * 관측결과에만 의존하지 않고 기존의 알고 있는 정보를 반영한다는 점에서 실제 인간의 학습 방식을 모사하기에 적합하다는 장점이 있지만, 사전확률을 모델링하는 것이 사후확률 정확도에 크게 영향을 미치는데 이러한 모델링이 어렵다는 단점이 있음
* 정보이론? 정보량? 엔트로피? 쿨백-라이블러 발산? 크로스 엔트로피?
    * 정보이론
        * 신호에 존재하는 정보의 양을 측정하는 이론
        * 정보이론의 핵심 아이디어는 잘 일어나지 않는 사건은 자주 발생하는 사건보다 정보량이 많다는 것
    * 정보량
        * 발생 가능한 사건의 확률분포에 음의 로그를 취한 수식
        * 정보이론의 핵심 아이디어와 매칭됨
    * 엔트로피
        * 정보량의 기댓값
        * 확률분포의 불확실성의 양을 나타내는 측도
    * 쿨백-라이블러 발산(KL Divergence)
        * 두 확률분포의 차이를 계산하는 데 사용하는 함수
            * 머신러닝 모델을 만들 때 데이터의 분포와 모델이 추정한 분포 간의 차이를 나타낼 수 있음
    * 크로스 엔트로피(Cross Entropy)
        * 두 확률분포가 교차로 곱해진 형태
            * 로그를 기준으로 양쪽에 위치한 확률분포가 다른 형태를 가지고 있음
        * 데이터의 확률 분포와 모델이 추정한 분포간의 크로스 엔트로피 = 데이터 확률 분포의 엔트로피 + 데이터의 확률분포와 모델이 추정한 분포의 쿨백-라이블러 발산
            * 데이터 확률 분포의 엔트로피를 상수 취급할 수 있기 때문에, 두 확률분포의 차이를 계산하는데 크로스 엔트로피를 활용할 수 있음
    

로지스틱 회귀(logistic regression)
* 이진 분류에서 널리 사용되는 모델(= 이진 분류기)
* 샘플이 특정 클래스에 속할 확률을 예측해서 제공함
    * 회귀의 결과 값이 양수이면 1을, 음수이면 0을 예측함
        * decision boundary는  **θ** · **x** = 0을 만족하는 **x**의 집합임
            * 경계가 선형임!
    * 속할 확률이 50% 넘는 클래스로 분류함
* 작동방식
    * p^ = sigmoid(regression(x))
        * p^은 예측확률을 의미함
        * regression은 linear regression을 의미함
    * 회귀를 통해 해당 훈련샘플의 logit(=log-odds)을 예측하도록 하고, 이러한 logit을 sigmoid 함수가 확률로 변환 시켜줌
        * sigmoid 함수와 logit은 역함수 관계이기 때문에 sigmoid(logit(훈련샘플))은 확률 값이 나오게 됨
        * logit = log(p / (1 - p))
        * odds = p / (1 - p)
            * p는 관심있는 사건이 일어날 확률
* 이진 분류의 비용 함수
    * binary cross entropy
        * MLE 관점에서 증명
            * ith target ~ Bernoulli(parameter = p^ = sigmoid(regression(xi)))
            * likelihood = P(ith target | xi ; **θ**) = sigmoid(regression(xi))^(ith target) * (1 - sigmoid(regression(xi)))^(1 - ith target)
                * **θ**는 regression parameters임
                * ith target ∈ {0,1}
            * binary cross entropy(= log loss, logistic loss)
                * J(**θ**) = -(1/N)log(총가능도) = -(1/N)Σ{(ith target) * log(sigmoid(regression(xi))) + (1 - ith target) * log(1- sigmoid(regression(xi)))}
                    * log는 단조 증가 성질이 있기 때문에 최적의 regression paramters를 찾는데 영향은 주지 않으면서, 계산을 유용하게 함
                    * -는 비용함수의 의미와 매칭하기 위해서 적용함
                        * 비용이 작을수록 성능이 좋다는 의미와 매칭
                    * 1/N은 전체 손실의 평균으로 값을 줄여주어 수치상 안정화됨
                * 경사 하강법을 통해 최적의 model parameters를 찾아냄

소프트맥스 회귀
* 다중 클래스를 분류하는데 널리 사용되는 모델
* linear regression을 클래스 종류 갯수만큼 사용하여 각 클래스에 대한 logit을 계산하고, 이러한 logit 집합에 소프트맥스 함수를 적용하여 각 클래스에 대한 확률을 추정함
* 추정 확률이 가장 높은 클래스로 분류함
* 다중 분류의 비용함수
    * Cross Entropy
        * MLE 관점과 정보이론 관점, 각 관점에서 도출 해낼 수 있음
            * MLE 관점
                * ith target ~ category(parameter = softmax regression)
                    * ith target ∈ {(1,0,...,0), (0,1,...,0), ..., (0,0, ..., 1)}
                * 기타증명은 logistic regression과 유사함
            * 정보이론 관점
                * 두 확률분포의 차이를 계산하는 쿨백-라이블러 발산을 크로스 엔트로피로 대체할 수 있음