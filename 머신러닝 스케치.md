머신러닝이란?
* 컴퓨터가 데이터로 학습할 수 있도록 프로그래밍하는 과학
* 컴퓨터가 학습하는 능력을 갖추게 하는 연구 분야
* 컴퓨터가 데이터로부터 학습하여 어떤 작업을 더 잘하도록 만드는 것

학습이란?
* 어떤 작업에서 주어진 성능 지표가 더 나아지는 것

머신러닝의 방향성
* 훈련 데이터로 학습하고 훈련 데이터에서 본 적 없는 **새로운 데이터**에서 좋은 예측(일반화)을 만들어야 함
* 훈련 데이터에서만 높은 성능을 내는 것은 좋지만 그게 전부가 아니다.
* 새로운 샘플에 잘 작동하는 모델을 만들어야 함

왜 머신러닝을 사용하는가?
* 전통적인 프로그래밍 기법은 판단하는 규칙이 점점 길고 복잡해지므로 유지 보수하기 힘들어진다.
* 음성인식과 같은 전통적인 프로그래밍으로 구현하기 힘든 문제에 적용 할 수 있다.
* 머신러닝 기반 프로그래밍은 자주 나타나는 패턴을 감지하여 판단하는데 좋은 기준을 자동으로 학습한다.
    * CNN 모델을 통해 MNIST 데이터셋 분류를 생각해보자. 숫자의 형태(패턴)를 감지하여 좋은 패턴(feature map 형성)을 기준 삼아 숫자를 판단함
* 그러므로 프로그램에 대한 유지 보수하기 쉬우며(단지, 새로운 데이터에 대해 학습 시켜줌. 필요시 알고리즘 업그레이드를 진행 할수도 있겠지만..) 대부분 정확도가 높다.
* 머신러닝 알고리즘이 학습한 것을 조사하여 예상치 못한 연관 관계나 새로운 추세, 겉으로는 보이지 않는 패턴을 발견하여 해당 문제를 더 잘 이해하도록 도와줌. 이를 **데이터 마이닝**이라고 함

머신러닝 시스템의 종류
* 학습하는 동안의 감독 형태나 정보량에 따라
    * 지도 학습, 비지도 학습, 준지도 학습, 강화학습
    * 지도학습
        * 훈련 데이터에 레이블이 있음
            * 레이블의 범주를 클래스라고 함
            * Target으로 지칭되기도 함
        * 분류(Classification), 회귀(Regression)
        * 예측 변수(Predictor Variable), 특성(Feature), 속성(Attribute)는 모두 같은 의미로 사용됨
        * 종류
            * K-nearest neighbors
            * linear regression
            * logistic regression
            * support vector machine
            * decision tree & random forest
            * neural networks(일부는 비지도, 준지도일 수 있음)

    * 비지도 학습
        * 훈련 데이터에 레이블이 없음
        * 종류
            * 군집(Clustering)
                * 비슷한것끼리 그룹으로 묶는 것
                * 알고리즘
                    * k-means
                    * DBSCAN
                    * hierarchical cluster analysis(HCA)
                    * outlier detection & novelty detection
                    * one-class SVM
                    * isolation forest
            * visualization & dimensionality reduction
                * visualization
                    * 데이터가 어떻게 조직되어 있는지 이해
                    * 예상하지 못한 패턴을 발견할 수 있음
                * Dimensionality Reduction
                    * 너무 많은 정보를 잃지 않으면서(구조를 최대한 유지) 데이터를 간소화
                    * feature extraction과 연관지을 수 있음.
                        * (TIP) 지도 학습 알고리즘에 데이터를 주입하기 전에 차원 축소 알고리즘을 사용하면.. 
                            1. 훈련 데이터의 차원을 줄임
                            2. 실행 속도가 빨라짐 
                            3. 디스크와 메모리에 차지하는 공간을 줄임
                            4. 경우에 따라 성능이 좋아짐

                * 알고리즘
                    * principal component analysis(PCA)
                    * kernel PCA
                    * locally-linear embedding(LLE)
                    * t-distributed stochastic neighbor embedding
            * association rule learning(연관 규칙 학습)
                * 데이터에서 특성 간의 흥미로운 관계를 찾음
                * 종류
                    * Aprioi
                    * Eclat

            * outlier detection + novelty detection = Anormaly detection
                * outlier detection
                    1. 이상치(ex> 이상한 신용카드 거래, 제조 결함)를 탐지 하는 알고리즘
                    2. 학습 알고리즘에 주입하기 전에 데이터 셋에서 이상치 제거

                * novelty detection
                    * 훈련 세트에 있는 모든 샘플과 **달라보이는** 샘플을 탐지

        * 준지도 학습
            * 레이블이 일부만 있는 데이터를 다룸
            * 대부분 지도 학습 + 비지도 학습의 조합

        * 강화 학습
            * agent action (in) environment -> reward OR penalty
            * 주어진 상황에서 가장 큰 보상을 얻기 위해(최상의 전략) 에이전트가 어떤 행동을 선택해야 할지(정책)를 스스로 학습함

* 입력 데이터의 스트림으로부터 점진적으로 학습 할 수 있는지 여부
    * 배치 학습과 온라인 학습
        * 배치 학습
            * 가용한 데이터를 모두 사용해 훈련함
            * 시간과 자원을 많이 소모하므로 먼저 시스템을 훈련시키고 그런 다음 제품 시스템에 적용하고 더이상 학습을 하지 않음(오프라인 학습)
            * 새로운 데이터를 학습하려면 새로운 데이터뿐만 아니라 이전 데이터도 포함한 전체 데이터로 처음부터 다시 훈련
    * 온라인 학습(-> 점진적 학습)
        * 데이터를 한 개씩 또는 미니배치 단위로 순차적으로 훈련
        * 매 학습 단계가 빠르고 비용이 적게 들어 데이터가 도착하는 대로 즉시 학습 가능
        * 나쁜 데이터에 의해서 시스템 성능이 점진적으로 감소할 수 있다는 것에 주의
        * 새로운 데이터에 대해서 얼마나 빠르게 적응할 지를 결정하는 학습률이 중요한 파라미터
            * 학습률(Learning Rate) : 경사하강법(학습 알고리즘)의 파라미터
                * 학습률이 높으면 시스템이 데이터에 빠르게 적응하지만, 예전 데이터를 금방 잊어버림
                * 학습률이 낮으면 느리게 학습하지만, 잡음이나 대표성이 없는 데이터에 덜 민감함
        * 학습이 끝난 데이터는 보관 할 필요가 없다면 버림
        * 빠른 변화에 적응해야 하는 시스템, 컴퓨팅 자원이 제한된 경우, 메인 메모리에 들어갈 수 없는 아주 큰 데이터셋 학습(외부 메모리 학습)에 적합


* 머신러닝 시스템이 어떻게 일반화 되는가에 따라
    * 사례 기반 학습과 모델 기반 학습
        * 사례 기반 학습
            * 시스템이 훈련 샘플을 기억하는 것이 곧 학습
            * 유사도 측정을 사용해서 새로운 데이터와 학습한 샘플 중 일부를 비교해서 분류

        * 모델 기반 학습
            * 모델을 만들어 예측함
            * 측정 지표 제시
                * 효용함수(utility function) : 모델이 얼마나 좋은지 측정 -> 최대화
                * 비용함수(cost function) : 모델이 얼마나 나쁜지 측정 -> 최소화

            * 작업 순서
                1. **데이터 분석**
                2. 모델 선택
                    * 모델은 관측한 것 중 간소화해서 경향성을 담고 있음
                        * 간소화란 새로운 샘플에 일반적이지 않을 것 같은 불필요한 세부사항을 제거하는 것
                    * 데이터에 관해 (불필요하다는 것에 대한)타당한 가정을 내리고 모델을 선택
                        * EX> 선형 모델은 데이터가 근본적으로 선형이고 샘플과 직선 사이의 거리는 무시할 수 있는 잡음이라고 가정
                3. 훈련 데이터로 모델 훈련(최적의 모델 파라미터를 찾음)
                4. 새로운 데이터로 모델을 통해 예측

머신러닝의 주요 도전 과제
* 나쁜 데이터
    * 충분하지 않은 양의 훈련 데이터
        * 충분한 데이터가 주어진다면 간단한 모델도 좋은 성능을 만들어냄
    * 대표성 없는 훈련 데이터
        * 훈련 데이터가 새로운 샘플을 잘 대표해야 일반화 잘될 것임.
        * 샘플이 작으면 샘플링 잡음(우연에 의한 대표성 없는 데이터)
        * 샘플이 많아도 샘플링 편향(표본 추출 방법이 잘못되어 대표성이 없는 데이터)

    * 낮은 품질의 데이터
        * 훈련 데이터에 에러, 이상치, 잡음이 많이 존재함.
        * **훈련 데이터 정제에 많은 노력 필요**
            * 이상치 : 샘플 제거 또는 수정(수동으로)
            * 결측값
                1. 결측값 있는 특성 자체 무시 
                2. 결측값 존재하는 샘플만 무시 
                3. 결측값 채움(평균값 등..)
    * 관련 없는 특성
        * 좋은 특성들을 찾아 훈련에 사용하자
        * feature engineering
            * 특성 선택 : 유용한 특성 선택
            * 특성 추출 : 특성을 결합하여 더 유용한 특성을 만듬
            * 특성 변형 : log(분포 변경), StandardScaler(스케일변경) 등


* 나쁜 알고리즘
    * 훈련 데이터에 과대적합(Overfitting)
        * 훈련 데이터에 너무 잘 맞지만 일반성이 떨어지는 현상
        * 훈련 데이터에 에러나 샘플링 잡음(대표성 없음), 관련 없는 특성과 같은 일반화와 관련 없는 요소들을 포함하고 있고, 이것들을 패턴으로 감지(학습)하면서 문제가 발생함
        * (caution) 과대적합은 훈련 데이터에 있는 모든 잡음의 패턴까지 모두 기억 할 만큼 모델이 너무 복잡할 때 일어남
            * 해결방법
                1. 모델을 간단화
                2. 모델에 제약(규제)
                3. 훈련 데이터를 더 수집(잡음에 덜 민감해짐)
                4. 훈련 데이터의 잡음 줄임
                    * EX> 오류 데이터 수정, 이상치(허용할 수 없는 잡음 보유 샘플) 제거
                5. 훈련 데이터의 특성 수를 줄임(관련 없는 것이 있거나 너무 많을 때)
                6. skewed target variable 확인
        * 훈련 오차가 낮지만, 검증 오차가 높다면 과대적합(학습곡선 측면)
        * 훈련 데이터에서 성능이 좋지만 검증 데이터에서 성능이 나쁘다면 과대적합(성능 측면)

        * 훈련 데이터의 질이 뛰어나다면 괜찮았겠지만, 현실에서 잡음이 없을 수가 없다는 것.
        * 데이터에 존재하는 잡음은 무시할 수 있다고 가정(일반화를 위해서)하여 데이터의 전체 구조를 모델이 학습 할 수 있도록 노력해야 하며, 잡음을 학습하지 않는 것이 중요

    * 훈련 데이터에 과소적합(underfitting)
        * 모델이 단순해서 데이터의 내재된 구조를 학습하지 못함
        * 해결방법
            1. 모델을 복잡화
            2. 모델의 제약을 줄임
            3. 훈련 데이터의 질을 높임(feature engineering)
            4. (TIP) **훈련 데이터를 더 수집하는 것은 의미가 없음**

        * 훈련 오차와 검증 오차 둘다 높다면 과소적합(학습곡선 측면)
        * 훈련데이터와 검증데이터 둘다 성능이 낮다면 과소적합(성능 측면)

테스트와 검증
* 데이터(세트) -> 훈련 데이터(세트) + 검증 데이터(세트) + 테스트 데이터(세트)
    * 훈련 데이터 : 모델을 훈련
    * 검증 데이터 : 모델 세부 튜닝, 모델 선택 기준 제시
    * 테스트 데이터 : 일반화 오차(**새로운 샘플**에 대한 오류 비율)에 대한 추정값을 테스트 데이터에서 모델을 평가함으로써 얻음

    * 검증 세트와 테스트 세트는 실전에서 기대하는 데이터를 가능한 잘 대표해야함

    * **테스트 세트의 성능을 높이려고 반복하는 작업**은 암묵적으로 테스트 세트에 최적화된 모델을 만드는 과정으로 새로운 데이터에 잘 작동하지 않을 수 있다.
        * 데이터 스누핑(data snooping) 편항

* 교차 검증(Cross Validation)
    * 훈련 데이터가 너무 작은데..
        * 검증 세트를 적게 잡으면..
            * 정확한 평가가 이루어지지 않음
        * 검증 세트를 크게 잡으면..
            * 모델이 훈련이 적절하게 이루어지지 않음

        * 교차 검증 방법으로 해결 할 수 있다.
    * 진행과정
        1. 작은 검증세트를 여러 개(fold) 만듬
        2. 검증 세트마다 나머지 데이터로 모델을 훈련하고, 해당 검증 세트로 평가
        3. 각각 검증 세트의 평가를 평균하여 성능 측정

    * (장점) 정확한 성능을 얻을 수 있지만, (단점) 시간이 오래걸림

* 데이터 불일치
    * 외부에서 얻은 데이터가 실제 제품 데이터를 대표하지 못함
    * 데이터 불일치 현상 대응 방법
        * 데이터 = (train set + train-dev set) + (validation set + test set)
            * (train set + train-dev set)
                * 외부에서 얻은 데이터
                * 외부에서 얻은 데이터의 validation set을 train-dev set이라고 생각
            * (validation set + test set)
                * 검증 세트와 테스트 세트가 실전에서 기대하는 데이터를 가능한 잘 대표해야함(이러한 데이터에 성능이 좋아야 하기 때문)
                * 검증 세트와 테스트 세트에 실전에서 기대하는 데이터를 배타적으로 포함
        * train-dev set에서 나쁜 성능
            * train set에 과대 적합
            * 과대 적합을 해결하도록 노력
        
        * train-dev set에서 좋은 성능, 검증 세트에서 나쁜 성능
            * 데이터 불일치
            * 외부에서 얻은 데이터가 실전 데이터와 일치 할 수 있도록 조작(전처리) 필요