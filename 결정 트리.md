결정 트리(decision tree)
* 시각적인 방법을 통해 해석이 용이하면서 비선형 데이터셋에서도 적용할 수 있는 모델
* 데이터 전처리가 요구되지 않음
* 훈련 세트의 회전에 민감함
    * 축이 바뀌는(회전하는) 것으로 이해함
* 훈련 데이터에 있는 작은 변화에도 매우 민감하게 반응함
    * node의 분할이 다르게 일어난다는 의미
* 분류에서 클래스 예측 및 확률 추정
    * 해당 샘플을 루트 노드부터 탐색해서 리프 노드를 찾아, 해당 리프 노드로부터 클래스 예측 또는 확률을 추정함
        * 확률은 해당 클래스의 훈련 샘플의 비율을 반환함
        * 클래스 예측은 확률이 가장 높은 클래스로 예측함
* 회귀에서 target value를 예측
    * 해당 샘플을 루트 노드부터 탐색해서 리프 노드를 찾아, 해당 리프 노드로부터 target value를 예측함
        * 예측값은 해당 리프노드에 존재하는 샘플들의 target value들의 평균 값임
* (사이킷런에서) CART(Classification And Regression Tree) 학습 알고리즘
    * 탐욕적 알고리즘
        * 당장 마주하는 노드 기준에서만 최적의 분할을 찾아냄
    * Tree의 형태가 이진트리임
    * 불순도(impurity)를 감소시키도록 자식 노드를 생성함
        * impurity measure
            * 분류
                * gini impurity(= gini index)
                    * G = 1 - Σpi^2
                        * i ∈ {1, ..., K}, K는 class의 개수
                * Entropy
                    * E = -Σpilog(pi)
                        * i ∈ {1, ..., K}, K는 class의 개수
                * gini impurity가 계산이 빠른 반면에, Entropy가 조금 더 균형 잡힌 트리를 만드는 경향이 있음
            * 회귀
                * MSE(Mean Square Error)
    * 비용함수
        * J(k, tk) = m_left / m * G_left + m_right / m * G_right
            * k는 임의의 특성, tk는 각 자식노드로 구분 짓는 threshold
            * m_left/m_right 는 자식노드의 샘플 수
            * m은 전체 샘플 수
            * G_left / G_right는 자식노드의 불순도
        * 특정 노드에서 비용 함수를 최소화 하는 feature 및 threshold 선택함
            * 비용함수를 자식노드들이 보유하고 있는 불순도의 가중평균으로 해석하면, 이러한 가중평균이 작을수록 자식노드들의 불순도가 감소했다는 측면으로 이해할 수 있음
            * 비용함수가 최소를 가지는 최적의 feature 및 threshold를 찾는 방법은 각 노드에서 모든 샘플의 모든 특성을 비교해서 찾음
* node importance
    * I(node) = (m_node / m) * G_node - J(selected_feature, selected_threshold)
* feature importance
    * I(ith feature) = ith feature에 의해 자식 노드를 생성한 node들의 importance 합 / 모든 node들의 importance합
    * normalized_I(ith_feature) = I(ith feature) / 모든 feature들의 importance 합
    * +) 랜덤 포레스트에서 feature importance는 각 트리에서의 I(ith feature)를 모두 더해서 평균 낸 것을 I(ith feature)로 사용함
* 규제 매개변수
    * 트리 모델은 제한을 두지 않으면 훈련 데이터에 과대적합되기 쉬움
        * 트리의 자유도를 제한(자식 노드를 생성하는 것에 제한)하는 규제가 필요함
    * 종류
        * min_ 으로 시작하는 매개변수를 증가시키거나 max_로 시작하는 매개변수를 감소시키면 규제가 커짐
        * max_depth
            * 트리의 최대 깊이를 제어
        * min_samples_split
            * 분할되기 위해 노드가 가져야 하는 최소 샘플 수
        * min_samples_leaf
            * 리프 노드가 가지고 있어야 할 최소 샘플 수
        * min_weight_fraction_leaf
            * (Minimum number of samples a leaf node should have is decided on)The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.
                * 리프노드의 절대적인 개수를 만족하기보단 일정 비율을 만족하도록 하는데 목적을 두고 있는 것 같음
                * ex> 500개의 샘플이 있고 가중치가 동일하다고 할 때, 이 매개변수를 0.01로 지정할 때 리프노드가 되기 위한 최소 샘플 개수는 5개임
            * 가중치는 sample_weight로부터, 비율은 사용자가 지정함
            * this is a way to deal with class imbalance.
                * 클래스의 비율이 다른데 최소 샘플 수와 같은 절대적인 개수를 조건으로 두는 것은 불합리 할 수 있음을 의미하는 것 같음
                * 소수 클래스 3개 채우는 것은 다수 클래스 3개 채우는 것보다 어렵기 때문에 소수 클래스가 1개 있는 것은 다수 클래스의 1개보다 더 가치 있도록 할 수 있을 것 같음
        * max_leaf_nodes
            * 리프 노드의 최대 수
        * max_features
            * 각 노드에 분할에 사용할 특성의 최대 수
            * 데이터셋의 특성 개수보다 작게 설정하면 무작위로 일부 특성이 선택됨
    * 규제 없이 트리를 훈련시키고 불필요한 노드를 pruning 하는 알고리즘도 있음
        * pruning 알고리즘 원리는 통계적으로 유의한지 판단
            * 카이제곱 검정을 활용한다고 함