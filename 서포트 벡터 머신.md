서포트 벡터 머신(SVM, Support vector machine)
* margin이라는 개념을 도입해 decision boundary를 정의해서 다양한 Task를 해결해내는 다목적 머신러닝 모델
    * 선형분류, 비선형 분류, 선형 회귀, 비선형 회귀, 이상치 탐색 등 활용 가능함
* (caution) SVM은 특성의 스케일에 민감함
    * 특성의 스케일을 조정하면 더 좋은 decision boundary가 형성됨
* 용어
    * decision boundary
        * 클래스를 구분 짓는 경계선
    * support vector
        * decision boundary와 가장 가까운 각 클래스의 샘플
    * margin
        * decision boundary와 평행하면서 각 support vector를 지나는 두 직선이 있을 때, 직선 간의 거리 또는 영역
* SVM 분류
    * SVM 분류는 margin 내부에 최대한 샘플이 존재하지 않으면서 margin을 최대화하도록 학습함
    * 하드 마진 분류? 소프트 마진 분류?
        * 하드 마진 분류(hard margin classification)
            * 모든 샘플을 올바르게 분류함
            * 데이터가 선형적으로 구분될 수 있어야 하며, 이상치에 민감함
                * 이상치에 따라서 decision boundary를 찾을 수 없거나 일반화를 하기 힘든 decision boundary를 형성함
        * 소프트 마진 분류(soft margin classification)
            * margin violation(margin 내에 샘플이 존재하거나 일부 클래스를 잘못 분류)를 일부 허용하면서 margin을 최대화 하는 decision boundary를 찾음
    * 선형 SVM 분류? 비선형 SVM 분류?
        * 선형 SVM 분류
            * 여기서 선형은 주어진 데이터 셋이 선형적으로 구분될 수 있음을 의미함
        * 비선형 SVM 분류
            * 여기서 비선형은 주어진 데이터 셋이 선형적으로 구분될 수 없음을 의미함
            * 선형적으로 구분되는 데이터 셋이 되도록 새로운 특성(ex> 특성의 거듭제곱 또는 특성 간에 교차항)을 추가하는 전략을 취함
                * sklearn.preprocessing의 PolynomialFeatures 활용
                    * 낮은 차수는 매우 복잡한 데이터 셋을 잘 표현하지 못할 수 있고, 높은 차수의 다항식은 굉장히 많은 특성을 추가하므로 모델이 느려짐
                * SVM의 커널 트릭 활용
                    * 실제로는 특성을 추가하지 않으면서 새로운 특성을 많이 추가한 것과 같은 결과를 얻을 수 있음
                    * kernel 종류
                        * poly
                            * 다항식의 특성을 추가하는 것과 같은 효과
                        * rbf
                            * 유사도 함수로 방사 기저 함수(rbf, radial basis funtion)을 활용해서 유사도 특성을 추가하는 것과 같은 효과
* SVM 회귀
    * SVM 회귀는 margin 외부에 최대한 샘플이 존재하지 않고 margin 내부에 최대한 샘플이 존재하도록 학습함
* 학습(pending..)
    * 제약이 있는 최적화
    * 쌍대 문제를 통해 커널 트릭을 어떻게 적용 해내었는지..?
