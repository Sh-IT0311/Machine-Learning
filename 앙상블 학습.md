앙상블 학습(ensemble learning)
* 여러 예측기(분류 또는 회귀 모델)를 앙상블이라고 부름
    * ex> 결정 트리의 앙상블을 랜덤 포레스트(random forest)라고 함
* 앙상블 학습은 여러 개의 예측기를 생성하고, 그 예측들을 결합함으로써 보다 정확한 예측을 도출하는 기법
* 성능이 좋지 못한 약한 학습기(weak learner)이라도 충분하게 많고 다양하게 있다면 강한 학습기가 될 수 있는 방법론
    * 단, 모든 약한 학습기가 독립적이고 그래서 오차에 상관관계가 없어야함
        * 오차에 상관관계가 있다는 의미는 같은 종류의 오차를 만들어내어 잘못된 클래스가 다수인 경우를 만들 수 있음
        * 여러 개의 약한 학습기를 독립적으로 만드는 방법은..
            * 각각 다른 종류의 모델(알고리즘)을 사용함
                * Ex> voting
            * 같은 알고리즘을 사용하면서 다른 데이터셋으로 다르게 학습시킴
                * Ex> bagging, pasting  
    * 일반적으로 앙상블의 결과는 하나의 예측기와 비교해서 편향은 비슷하지만 분산은 줄어듬
        * 다양한 의견들을 통해 결론을 내리기 때문에 예측 변동폭이 줄어 분산은 줄어든다고 이해함
* 종류
    * voting(투표 기반 분류기)
        * 서로 다른 알고리즘을 가진 여러 분류기들의 투표를 통해 최종 예측 결과를 결정하는 방식
            * 분류는 통계적 최빈값, 회귀는 평균으로 접근함
            * 학습 및 예측을 병렬로 수행할 수 있기 때문에 확장성에 용이함
        * hard voting
            * 서로 다른 알고리즘을 가진 여러 분류기들의 예측 값들을 다수결로 최종 class를 결정함
        * soft voting
            * 서로 다른 알고리즘을 가진 여러 분류기들의 예측 결과값간 확률을 평균하여 높은 확률을 가지는 class를 최종 class로 결정함
            * hard voting보다 성능이 우수함
    * bagging 과 pasting
        * 공통점
            * 같은 알고리즘을 사용하고 훈련 세트의 서브셋을 무작위로 구성하여 각 분류기에 각기 다른 서브셋을 통해 학습시키는 것
            * 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식이 voting과 똑같음
                * 분류는 통계적 최빈값, 회귀는 평균으로 접근함
                * 학습 및 예측을 병렬로 수행할 수 있기 때문에 확장성에 용이함
        * 차이점
            * bagging은 훈련 세트의 서브셋을 구성할 때 중복을 허용하여 샘플링함(= bootstrapping)
                * 부트스트래핑은 서브셋에 다양성을 증가시킴
                    * 중복을 허용한 샘플링이 다양성을 증가시키는가?
                        * 중복을 허용하면서 더 다양한 조합을 가지는 서브셋을 구성하는 효과를 가져온다고 이해함
                    * 증가된 다양성을 통해 배깅이 페이스팅보다 편향은 높지만 예측기들 간에 상관관계를 줄이므로 분산을 감소시킴
                * oob(out of bag)
                    * 부트스래래핑 특성상, 어떤 샘플은 여러 번 샘플링 되고 어떤 샘플은 전혀 선택되지 않는데 선택되지 않는 샘플을 oob 샘플이라고 함
                    * 훈련 되는 동안 oob 샘플을 사용하지 않으므로 별도의 validation set을 구성하지 않고 oob 샘플을 사용해 평가 할 수 있음
                * Random Forest
                    * Decision Tree + Bagging
                    * Feature Importance
                        * 랜덤 포레스트에서 feature importance는 각 트리에서의 I(ith feature)를 모두 더해서 평균 낸 것을 I(ith feature)로 사용함
                        * Decision Tree에서는 일부 특성이 배제되는 반면에 Random Forest는 거의 모든 특성에 대해 평가할 기회를 가지고 있음
                    * VS Extra Trees
                        * ※주의※ : 앙상블임
                        * Random Forest와 차이점은
                            1. Whole Origin Data 사용함
                            2. Split 할 features를 무작위로 선정하고, 이러한 features에서 최적의 feature를 찾음
                                * 무작위성으로 인해 Bias 증가, Variance 감소
                                * 탐색할 Feature가 줄어들기 때문에 연산속도 증가함
            * pasting은 훈련 세트의 서브셋을 구성할 때 중복을 허용하지 않고 샘플링함
        * 전반적으로 배깅이 페이스팅보다 더 나은 모델을 만듬
            * 물론, 시간과 computing power가 충분하다면 배깅과 페이스팅을 직접 비교하고 선택하는 것이 좋음
        * 서브셋을 구성하는 단위가 샘플이라면, 샘플을 구성하는 features를 샘플링 하는 특성 샘플링으로도 접근 할 수 있음
            * 특성 샘플링은 더 다양한 예측기를 만들어 편향을 늘리는 대신 예측기들 간에 상관관계를 줄이므로 분산을 감소시킴
    * boosting
        * 약한 학습기들을 직렬로 연결하여 앞의 모델을 보완해나가면서 일련의 약한 학습기를 학습하여 강한 학습기로 만드는 앙상블 방법
            * 순차적으로 학습이 이루어지기 때문에 병렬 연산이 불가능함
                * XGboost의 경우, 각 약한 학습기를 학습할 때 병렬 연산을 함
            * 두 가지 관점의 Loss Function이 존재함
                * boosting 선형결합에 대한 Loss Function
                    * ex> adaboost <-> exponential loss function
                * 각 약한 학습기의 Loss Function
                    * ex> logistic regression <-> binary cross entropy
        * adaBoost(adaptive boosting)
            * **가중치를 이용하여 이전 모델의 약점을 보완**하는 새로운 모델을 순차적으로 이어붙여 최종적으로 직렬을 이루는 모델들의 선형 결합을 통해 강한 학습기를 생성시키는 알고리즘
                * 모델의 약점을 보완한다?
                    1. 샘플의 가중치 재구성
                        * 이전 모델이 과소적합했던 샘플의 가중치를 크게함
                        * 반대로 잘 판별한 샘플의 가중치를 낮춤
                    2. 학습 데이터 재구성
                        * **가중치에 비례한채로 복원 추출을 해서 다음 모델의 학습 데이터를 재구성함**
                            * 가중치가 높은 샘플들이 많이 포함되서 이전 모델이 과소적합 했던 샘플들을 더 잘 예측하도록 견인함
                        * 새로운 학습 데이터에 대해서 가중치는 1/n으로 리셋함
                            * n은 학습 데이터 수
                        * +) 복원 추출을 통해 학습 데이터를 재구성하는 방법 말고 weighted gini index를 이용하는 방법도 있다고 함
                * 각 약한 학습기의 가중치가 샘플의 가중치 및 최종 예측에 영향을 많이 미침
                    * **주어진 데이터를 잘 예측한 학습기일수록 가중치가 높음**
            * 특징
                * 장점
                    * 과적합에 영향을 덜 받음
                        * 물론 약한 학습기의 개수가 많아지면 과적합이 발생하겠지만 상대적으로 늦게 발생함
                        * 과적합이 발생하면 학습기의 수를 줄이거나 규제를 강화시킴
                    * 약한 학습기만 잘 구현 되어 있다면 알고리즘 구현이 쉬움
                    * 약한 학습기에 대한 제한이 없음
                        * 결정트리 이외에 모델을 사용 할 수 있음
                * 단점
                    * 이상치에 민감함
                    * 앙상블의 공통적인 문제이지만, 입력 변수와 출력 변수간의 관계를 해석하기 어려움
            * adaboost classifier 
                * 약한 학습기가 결정트리라면 max_depth를 보통 1로 사용함
            * adaboost regressor
                * 약한 학습기가 결정트리라면 max_depth를 보통 3으로 사용함
        * gradient (descent) boosting
            * 그레이디언트(gradient)를 이용하여 이전 모델의 약점을 보완하는 새로운 모델을 순차적으로 이어붙여 최종적으로 직렬을 이루는 모델들의 선형 결합을 통해 강한 학습기를 생성시키는 알고리즘
                * **손실함수를 줄이는 negative gradient를 모델의 파라미터의 업데이트가 아닌 negative gradient를 다음 모델이 학습하고 선형결합 하는 Boosting으로 접근함**
                    * **특정 Task에 해당하는 loss function의 negative gradient를 이용하여 유연하게 gradient boosting을 진행 할 수 있음**
            * 특징
                * table format data에 성능이 좋고, 머신러닝 알고리즘 중에서 성능이 높다고 알려짐
                * bias는 상당히 줄일 수 있어도 과적합이 쉽게 일어나는 단점이 있음
                    * 약한 학습기에 대한 regularization이 요구됨
                    * early stopping 기법이 요구됨
                    * 각 학습기가 훈련할 때 사용할 훈련 샘플을 무작위 샘플링을 활용 할 수 있음 
                        * 따로 stochatic gradient boosting으로 명명하기도 함
                * learning rate를 낮게 설정하면 학습 시간은 오래 걸리지만 일반적으로 성능이 좋아짐
                    * learning rate와 n_estimator는 trade-off 관계임
                * 개선된 GBM 모델마다 leaf-wise 또는 level-wise 방식을 채택함
                    * 이러한 방식 차이는 학습되는 Node의 순서를 결정하는데,이러한 특징은 early stopping이나 pruning에서 매우 다른 트리를 만들어냄
                    * leaf-wise = criterion first = best-first
                    * level-wise = depth-first
            * 개선된 GBM
                * XGBoost
                    * 병렬 처리 지원
                        * 각 약한 학습기(Tree)의 학습을 병렬 연산 처리해서 학습 속도를 향상시킴
                    * leaf-wise 방식 대신에 level-wise 방식을 채택함
                        * During the tree building process, XGBoost automatically stops if there is a node without enough cover (the sum of the Hessians(Second Partial Derivative) of all the data falling into that node) or if it reaches the maximum depth.
                        * Information Gain이 없는 가지 밑에서도 Information Gain이 있는 가지가 생겨날 수 있다는 가정에 기반함 
                    * 가지치기(pruning)
                        * After the trees are built, XGBoost does an optional 'pruning' step that, starting from the bottom (where the leaves are) and working its way up to the root node, looks to see if the gain falls below gamma (a tuning parameter). If the first node encountered has a gain value lower than gamma, then the node is pruned and the pruner moves up the tree to the next node. If, however, the node has gain higher than gamma, the node is left and the pruner does not check the parent nodes.
                    * 결측치 처리
                        * Compute the value to be assigned to the prediction at each child node using all the data, and chose the assignment that minimizes the loss.
                        * Automatically "learn" what is the best imputation value for missing values based on reduction on training loss.
                    * regularization
                        * In tree-based methods regularization is usually understood as defining a minimum gain so which another split happens. 
                        * Minimum loss reduction(=gain) required to make a further partition on a leaf node of the tree. 
                        * The larger gamma is, the more conservative(보수적인) the algorithm will be.
                * LightGBM
                    * leaf-wise 방식 채택함
                    * histogram-based algorithm
                        * 효율적으로 Tree의 분할점을 찾음
                        * original tree는 pre-sorted algorithm을 사용함
                    * GOSS(Gradient-based One-Side Sampling)
                        * 데이터 수를 줄임
                    * EFB(Exclusive Feature Bundlung)
                        * 데이터 차원을 줄임
                    * Overfitting에 취약함
                        * 데이터의 수가 10000개 이상일 때 사용하도록 권장함
                * Catboost
                    * Target Leakage
                        * Conditional Shift
                        * Predicttion Shift
                    * Ordering Principle
                        * Ordered Boosting
                            * 다른 데이터로 학습한 모델로 잔차를 갱신함
                        * Ordered Target Encoding
                            * 다양한 Target Statistic을 생성하도록 함
                                * Overfitting을 방지하도록 함
                            * Cardinality가 낮으면 One-Hot Encoding도 시행함
                    * level-wise
                    * Feature Combinations
                        * all Feature들을 Greedy 방식으로 조합함
    * stacking
        * 스태킹(Stacking)은 학습된 개별 모델이 예측한 결과를 훈련 스태킹(메타) 데이터 세트로 만들어 별도의 메타 모델을 통해 다시 학습을 수행하고 테스트 데이터 또한 개별 모델을 통해 테스트 스태킹 데이터 세트로 변환해 메타 모델로 최종 예측을 수행하는 방식
            * 스태킹은 두 종류의 모델이 필요함
                * 개별적인 기반 모델
                * 메타 모델(블렌더)
            * 스태킹의 핵심 아이디어는 학습된 개별 모델들의 **예측 결과들을 스태킹**하여 메타모델의 학습용 스태킹 데이터 및 테스트 스태킹 데이터 세트를 만드는 것임
            * 실제 구현은.. 
                1. Cross Validation을 활용해서 훈련 스태킹 데이터와 테스트 스태킹 데이터를 생성함
                    * 3개의 Folds로 생각했을 때, 아래의 과정은 train Folds와 validation Fold 번갈아 변경되면서 3번 반복됨
                        * 2개의 Folds로 개별 모델들을 학습
                        * 학습된 개별 모델들로 나머지 1개의 Fold는 학습용 스태킹 데이터를 생성함
                        * 학습된 개별 모델들로 테스트 스태킹 데이터를 생성함
                            * 테스트 스태킹 데이터는 CV 반복 횟수 만큼(3개) 생성되기 때문에 평균을 하여 최종 테스트 스태킹 데이터를 생성함
                2. 훈련 스태킹 데이터와 원본 데이터의 레이블을 합쳐서 메타 모델을 학습하고, 테스트 스태킹 데이터로 예측을 수행한 뒤 원본 테스트 데이터의 레이블과 비교해 평가를 진행함
            * 여러 개의 튜닝된 개별 모델의 요구되며, 일반적으로 성능이 비슷한 모델을 결합해 좀 더 나은 성능 향상을 도출하기 위해 적용함




 